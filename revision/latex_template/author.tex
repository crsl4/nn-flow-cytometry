%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% track changes
\usepackage[normalem]{ulem}
\usepackage[dvipsnames]{xcolor}
\newcommand{\claudia}[2]{\sout{#1}\textcolor{magenta}{\textbf{#2}}}
\newcommand{\missing}[1]{\textcolor{red}{\textbf{#1}}}


\begin{document}

\title*{Prediction of functional markers of mass cytometry data via
  deep learning}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
%\thanks{Both authors contributed equally to this manuscript}
\author{Claudia Sol\'is-Lemus, Xin Ma, Maxwell Hostetter II, Suprateek
  Kundu, Peng Qiu, Daniel Pimentel-Alarc\'on}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Claudia Sol\'is-Lemus \at Emory University, Atlanta, GA, \email{csolisl@emory.edu}
\and Xin Ma \at Emory University, Atlanta, GA,
\email{xin.ma@emory.edu}
\and Maxwell Hostetter II \at Georgia State University, Atlanta, GA,
\email{mhostetter1@student.gsu.edu}
\and Suprateek Kundu \at Emory University, Atlanta, GA,
\email{suprateek.kundu@emory.edu}
\and Peng Qiu \at Georgia Institute of Technology and Emory University, Atlanta, GA,
\email{peng.qiu@bme.gatech.edu}
\and Daniel Pimentel-Alarc\'on \at Georgia State University, Atlanta, GA,
\email{pimentel@gsu.edu}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\abstract{Recently, there has been an increasing interest in the
  analysis of flow cytometry data, which involves measurements of a
  set of surface and functional markers across hundreds and thousands
  of cells. These measurements can often be used to differentiate
  various cell types and there has been a rapid development of
  analytic approaches for achieving this. However, in spite of the
  fact that measurements are available on such a large number of
  cells, there have been very limited advances in deep learning
  approaches for the analysis of flow cytometry data. Some preliminary
  work has focused on using deep learning techniques to classify cell
  types based on the cell protein measurements. In a first of its'
  kind study, we propose a novel deep learning architecture for
  predicting functional markers in the cells given data on surface
  markers. Such an approach is expected to automate the measurement of
  functional markers across cell samples, provided data on the surface
  markers are available, that has important practical advantages. We
  validate and compare our approach with competing machine learning
  methods using a real flow cytometry dataset, and showcase the
  improved prediction performance of the deep learning architecture.}

\section{Introduction}

% from Peng Qui PQ.docx: ---
Multiparametric single-cell analysis has advanced our understanding of
diverse biological and pathological processes, providing insights into
cellular differentiation, intracellular signaling cascades and
clinical immunophenotyping. Modern flow cytometers typically provide
simultaneous single-cell measurements of up to 12 fluorescent
parameters in routine cases, and analysis of up to 30 protein
parameters has been recently made commercially available. In addition,
a next-generation mass cytometry platform (CyTOF) has become
commercially available, which allows routine measurement of 50 or more
protein markers.

Despite the technological advances in acquiring an increasing number
of parameters per single cell, approaches for analyzing such complex
data lag behind. The existing approaches are often subjective and
labor-intensive. For example, the widely used gating approach
identifies cell types by user-defined sequences of nested 2-D
plots. There have been efforts to develop clustering algorithms (e.g.,
flowMeans\cite{Aghaeepour2010}, flowSOM\cite{VanGassen2015},
X-shift\cite{Samusik2016}, and dimension reduction algorithms
(e.g., SPADE \cite{Qiu2011}, tSNE\cite{vanderMaaten2008},
Scaffold\cite{Spitzer2015}. However, there is still huge space
for developing new methods to ask new questions in this field.
% ---

Recently, deep learning models are revolutionizing the fields of
precision medicine, data mining, astronomy, human-computer
interactions, among many others, by becoming a major discovery force
in science due to the unprecedented accuracy in prediction.
\claudia{Moreover}{However}, \claudia{}{despite} deep learning
approaches \claudia{have shown}{showing} accurate performance on
genomics and biomedical
applications\cite{Ciresan2013,Denas2013,Fakoor2013,Leung2014,CruzRoa2014,Li2017,Mobadersany2018},
\claudia{}{the CyTOF community has not fully adopted these
  methods yet.}  \claudia{Furthermore}{In fact, it turns out that}
CyTOF data is perfectly suited for deep learning methods.  On one
side, identify markers define a cell type (e.g., B cell, T cell,
monocytes, MSC), and on the other side, expressions of functional
markers identify the cell's activity (e.g., quiescent, secreting
cytokines, proliferating, apoptosis). Since CyTOF technology allows
for the simultaneous measurement of a large number of protein markers,
most CyTOF studies measure both identity markers and functional
markers, providing data for supervised learning tools, like neural
networks.  In addition, each CyTOF run typically collects data on
$10^6$ cells, creating an ideal large dataset in which the number of
samples (cells) is orders of magnitude larger than the number of
variables (markers). Deep learning methods are particularly suited for
this type of big data.

In terms of motivation, there are two main reasons to predict the
functional markers from surface markers in CyTOF data: 1) monetary and
time cost, and 2) technical limit of the total number of markers CyTOF
can measure, which is currently around 50 protein markers. That is, if
we can accurately predict some functional markers based on surface
markers, there is no longer the need to include those functional
markers in the staining panel (experimental design), and thereby
freeing up channels to measure more surface markers or additional
functional markers that cannot be predicted.


Here, we \claudia{}{present the first work to} explore neural network
models to predict functional markers (internal phosphoproteins) with
identify markers (cell surface proteins). We compare the performance
of neural networks in terms of accuracy and speed to other standard
statistical approaches like regression, and random forests.  We show
that neural networks improve prediction of functional markers, making
them a powerful alternative to the usual regression techniques,
\claudia{}{thus, providing quantitive evidence that deep learning
  methods can enrich the existing research landscape of CyTOF big
  data.}



\section{Data}

\subsection{Pre-processing}

The CyTOF dataset has been previously published in
\cite{Bendall2011,Qiu2011}. It contains single-cell data for 5 bone marrow samples from healthy donors. The data for each sample contains measurements for 31 protein markers for individual cells, including 13 cell surface markers which are conventionally used to define cell types, as well as 18 functional markers which reflect the signaling activities of cells. The number of cells per sample is roughly 250,000, and the total number of cells across all 5 samples is 1,223,228. Thus, the data can be expressed in a $1223228 \times 31$
matrix.

The data was transformed with inverse hyperbolic sine function
($arcsinh$ with co-factor of 5), which is the standard transformation
for CyTOF data \cite{Bendall2011}.


\subsection{Exploratory analysis}

We will compare the performance of different methods (explained next
section) to predict the functional markers with the surface markers.
The data is highly complex and correlated, violating some of the
fundamental assumptions of standard statistical approaches (like
regression). For example, the data is highly skewed and the pattern
between response and predictor is not linear (see figure
\ref{exploratory}).

\begin{figure}
\includegraphics[scale=0.3]{figures/surface-hist.pdf}
\includegraphics[scale=0.3]{figures/functional-hist.pdf}
\includegraphics[scale=0.3]{figures/scatterplot.pdf}
\caption{Exploratory plots of surface and functional markers. The
  histograms show a biased pattern, and the scatterplot shows non
  linearity, both violations of crucial assumptions in standard
  regression models.}
\label{exploratory}
\end{figure}


\section{Materials and Methods}

\subsection{Background on neural networks}

A neural network model is formed by several {\em neurons}. Each neuron
receives an input vector $x$, then weights its components according to
the neuron's weight vector $w$, adds a {\em bias} constant $b$, and
passes the result through a non-linear {\em activation function}
$\sigma$. This way, the output of a neuron is given by $\sigma(w^Tx +
b)$. There are several options for the activation function
$\sigma$. Common choices include the {\em sigmoid} function
$\sigma(z)=\frac{1}{1+e^-z}$ or the {\em rectified linear unit} (ReLU)
$\sigma(z)=\max(0,z)$. For the CyTOF data, we use the hyperbolic
tangent as activation function, as it showed better performance than
the sigmoid or ReLU functions (more details on the specific neural
network fit in subsection \ref{comparison}).

The final output of the network is given by $\hat{f}(x)$ with
parameters $\mathbf{W}_1,\cdots,\mathbf{W}_L$ for the weight matrices
and $b_1,\cdots,b_L$ for the bias vectors for each layer.

The estimation of the parameters is done through the following
optimization
\begin{equation}
\min_{\{W_l,b_l\}_{l=1}^L} \ \ \sum_{i=1}^n \| y_i-\hat{f}(x_i) \|^2.
\end{equation}

The most widely used technique to solve this optimization is through
stochastic gradient descent (SGD) and back-propagation, but we
discovered that Adam\cite{Kingma2014}, an algorithm for first-order
gradient-based optimization of stochastic objective functions, based
on adaptive estimates of lower-order moments had better performance
for our data (more details in subsection \ref{comparison}).


\subsection{Methods comparison}
\label{comparison}

We fit a neural network model to predict functional markers from
surface markers, and compare its performance to three classical
statistical methods: 1) linear regression (unpenalized and penalized),
2) decision trees, and 3) random trees. Due to computational time
constraints, we could not fit a support vector regression (SVR) model
(more in Discussion). We compared the performance of the four
approaches by computing the mean square error (MSE) of the predicted
responses.

To fit the models, we divided the data into training set,
\claudia{validation set}{} and testing set. The training set was used
to \claudia{fit each of the four models}{perform a 10-fold cross
  validation scheme to choose the best model fit for each method. We
  chose the model fit with the smallest average MSE}. \claudia{The
  validation set was used to determine the best setup (tuning
  parameters) of each model in terms of MSE. Finally, the test set was
  used to compare the MSE across methods.}{Then, we used all the
  training samples to estimate the final model for each method
  (e.g. model and tuning parameters), and reported the prediction MSE
  of each method based on the testing samples.}

The complete data consisted of 1,223,228 cells in 18 functional
markers (responses) and 15 surface markers (predictors), which we
divided as follows: \claudia{750,000}{ 1,000,000} rows as training set,
\claudia{250,000 rows as validation set}{}, and 223,228 rows as
testing set.

We used two separate measure of performance: a vector MSE (equation
\ref{vectorMSEformula}) and individual MSE (equation
\ref{MSEformula}), one per predictor (so, 18 in total).

The vector MSE is defined as
\begin{equation}
MSE_{vec} = \frac{1}{2n} \sum_{i=1}^n ||\hat{Y}_i - Y_i ||^2_2
\label{vectorMSEformula}
\end{equation}
where $\hat{Y}_i \in \mathbf{R}^{18}$ is the predicted vector of
responses for individual $i$, and $Y_i \in \mathbf{R}^{18}$ is the
observed vector of responses for individual $i$.

The individual MSE for predictor $k$ is defined  as
\begin{equation}
MSE_{(k)} = \frac{1}{2n} \sum_{i=1}^n (\hat{Y}_{k,i} - Y_{k,i} )^2
\label{MSEformula}
\end{equation}
where $\hat{Y}_{k,i} \in \mathbf{R}$ is the $k^{th}$ predicted
response ($k=1,\cdots,18$) for individual $i$, and $Y_{k,i} \in
\mathbf{R}$ is the $k^{th}$ observed response for individual $i$.

% https://github.com/pluskid/Mocha.jl/blob/master/docs/user-guide/solver.rst
\vspace{0.25cm}
\noindent \textbf{Neural network model:} We tested different network
architectures, activation functions, regularization coefficients,
solver methods, momentum policies, and learning rates with 50,000
maximum epochs. The best network has four layers (see figure \ref{nn})
with 90, 90, 45 and 45 nodes. The network uses hyperbolic tangent as
activation function, regularization coefficient of 0.0001, momentum
policy fixed at 0.8, inverse-decay learning rate policy with base
learning rate, gamma and power parameters at 0.01, 0.0001, 0.75. We
used Adam solver\cite{Kingma2014}, an algorithm for first-order
gradient-based optimization of stochastic objective functions, based
on adaptive estimates of lower-order moments, instead of Stochastic
Gradient Descent (SGD) as the former showed increased accuracy. All
networks were trained using the julia package
\texttt{Mocha}\cite{Bezanson2017,mocha}.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/neuralNet.eps}
\caption{Neural network for predicting functional markers (18
  responses) from surface markers (15 predictors) with 4 hidden
  layers with 90, 90, 45 and 45 nodes each.}
\label{nn}
\end{figure}

\vspace{0.25cm}
\noindent \textbf{Linear regression (unpenalized/penalized):} We fit
standard linear regression, as well as the penalized version with
LASSO penalty under different penalization parameters. We used the
\texttt{ScikitLearn}\cite{scikit} julia wrapper, with default
settings. We noted that the penalized version performed worse than the
unpenalized version for all the predictors (regardless of penalty
parameter), so we only present results below for the unpenalized
linear regression model.

\vspace{0.25cm}
\noindent \textbf{Decision tree and random forest regressions:} We fit one decision tree
regression per response with \texttt{ScikitLearn} julia wrapper, with default
settings. We compared the performance of the ``mse'' criterion and the
Friedman's improvement score, deciding on the former (``mse'') which
is the default setting. We did not constraint the maximum depth of the
tree, and set as 2 the minimum number of samples required to split an
internal node. In addition, we did not constraint the maximum number
of features to consider when looking for the best split. Later, we fit
20 trees into a random forest regression. We could not explore more than 20
trees due to computational time constraints.



\section{Results}

Figure \ref{vectorMSE} (left) shows the vector MSE (equation
\ref{vectorMSEformula}) across all four different methods, being
decision tree the least accurate and neural network the most
accurate. Figure \ref{vectorMSE} (right) shows a comparison on
computation time (in seconds) among the four methods, being linear
regression the fastest and random forest the slowest. To sum up, the
neural network approach outperforms the other three methods in terms
of prediction accuracy, without sacrificing too much computational
speed.


\begin{figure}
\centering
\includegraphics[scale=0.35]{figures/vector-mse.pdf}
\includegraphics[scale=0.35]{figures/time.pdf}
\caption{Left: Vector MSE (equation \ref{vectorMSEformula}) for all
  four methods sorted from most accurate (neural network) to least
  accurate (decision tree). Right: Running time (in seconds) for the
  training and validation sets (sample 750,000 rows) for all four
  methods, sorted from fastest (linear regression) to slowest (random
  forest).}
\label{vectorMSE}
\end{figure}


Figure \ref{MSE} shows the individual MSE (equation \ref{MSEformula})
per response (18 responses in x-axis) for each of the four
methods. Again, the prediction accuracy of neural network is better
than the other three methods for all the 18 predictors.

\begin{figure}
\centering
\includegraphics[scale=0.45]{figures/mse.pdf}
\caption{Individual MSE (equation \ref{MSEformula}) for each 18
  responses. The neural network outperforms all other methods across
  all responses.  Lines are drawn simply for visual effect.}
\label{MSE}
\end{figure}

The MSE performance varies across responses. For example, the first response (functional marker 141.pPLCgamma2) has an overall MSE lower than other responses like the third (functional marker 152.Ki67), the 8th (functional marker 159.pSTAT3) or the 14th (functional marker 171.pBtk.Itk).

Figure \ref{violin} (left) shows the violin plots for these 4 functional markers. We observe that the 14th response has a wider range and heavier tails than the other responses, which is confirmed in the scatterplots on the center and right (figure \ref{violin}). It appears that the wider spread and higher variability of the 14th response (functional marker 171.pBtk.Itk) causes the lower prediction accuracy compared to other responses, like the first one (functional marker 141.pPLCgamma2).

\begin{figure}
\centering
\includegraphics[scale=0.3]{figures/violin.pdf} \hspace{0.25cm}
\includegraphics[scale=0.3]{figures/response1.pdf}
\hspace{0.25cm}
\includegraphics[scale=0.3]{figures/response14.pdf}
\caption{Left: Violin plot for four functional markers (responses). Horizontal lines represent the 25th quantile, median and 75th quantile. Center: Predicted vs observed responses on the first functional marker (141.pPLCgamma2)
  across all four methods. Left: Predicted vs observed responses on the 14th functional marker (171.pBtk.Itk)
  across all four methods. The closer the slope to 1 (black line), the
  better.}
\label{violin}
\end{figure}

Finally, we present selected scatterplots of surface markers as predictors for the responses 1,3,8 and 14 (figure
\ref{predObs}). We can appreciate in these plots the non-linear relationship between the predictors and responses, which justifies the use of a neural network approach.

\begin{figure}
\centering
\includegraphics[scale=0.3]{figures/scatterplot1.pdf} \hspace{0.5cm}
\includegraphics[scale=0.3]{figures/scatterplot14.pdf}
\caption{Scatterplot of selected surface markers (predictors) and selected functional markers (responses). Left: the first response (functional marker 141.pPLCgamma2) shows a linear relationship to the predictor (surface marker 115.CD45), which partially explains the better MSE in figure \ref{MSE}. Right: the 14th response (functional marker 171.pBtk.Itk) shows a non-linear relationship to the predictor (surface marker 115.CD45), which partially explains the worse MSE in figure \ref{MSE}.}
\label{predObs}
\end{figure}



\section{Discussion}

\claudia{}{To the best of our knowledge, our work presents the first
  neural network model for the prediction of functionality in CyTOF
  big data.}  In this work, we showed that a neural network model
outperforms standard statistical approaches like linear regression and
random forest in the prediction of functional markers from surface
markers for CyTOF data. Neural networks were also faster and more
efficient than random forests, which make them a more viable choice
for big datasets.

The improved prediction accuracy of neural networks can be explained
by their flexibility to account for non-linearity or skewness.  Unlike
regression models, neural networks do not have linearity or normality
assumptions, and they take advantage of the correlation structure
among responses by fitting a network for the whole response vector.
\claudia{}{It is important to note that the while the neural network
  model shows better performance than the other methods, this edge is
  still quite modest. Random forest models also represent a suitable
  choice for prediction under non-linear assumptions. However, we show
  here that when scalability also matters, the neural network model
  outperforms random forests (in computation time), while providing
  more accurate predictions. We claim that neural networks are better
  suited for big CyTOF data than random forests, but more work is
  needed to corroborate this claims.}

As mentioned already, CyTOF data is perfectly suited for deep learning
methods given the simultaneous measurement of a large number of
protein markers, including both identity markers and functional
markers. Both measurements allow for the implementation of highly
accurate supervised methods, like neural networks. In addition, the
structure of CyTOF data is ideal for deep learning: number of samples
orders of magnitude greater than the number of variables.

The accuracy in the prediction of functional markers from surface
markers has economic and computational advantages, for example,
considering the limitation to the total number of markers CyTOF can
measure, which is currently around 50 protein markers. Being able to
predict functional markers from surface markers could allow for
different types of staining panels which could measure more surface
markers, or focus on functional markers not so easily predicted.

\claudia{}{Initially, we were surprised by the lack of scalability of
  SVR models. However, given the non-linearity and optimization
  complexity\cite{Smola2003, Welling2004}, it appeared that our
  dataset was too large for the computation of the kernel matrix,
  which is $n \times n$ where $n$ is the number of samples (more than
  1 million in our case). More work is needed to assess the
  performance of SVR under smaller datasets to assess the accuracy of
  such models and the limits in scalability.}

For future work, we can include an extended version of the
dataset\cite{Bendall2011,Qiu2011} that includes 24 healthy sample of
bone marrow treated by 24 different drugs. In this setting, we are
interested in predicting the functional markers under different drug
scenarios, using information at baseline (no treatment) and surface
markers at different treatment levels.
Furthermore, based on the trained deep learning model, we are
interested in the question of whether we can identify cell clusters,
and whether these cell clusters agree with well-accepted cell types in
literature.
Finally, if we focus on cells belonging to the same known cell type, and
examine the distribution of functional markers and the correlation
with the subtle variations of the identity markers among cells of this
type, we can explore whether there is evidence that the specific cell
type could be further divided into subtypes.


\vspace{0.5cm}
\noindent \textbf{Acknowledgements:} We thank the editor as well as
two anonymous reviewers whose suggestions greatly improved the
manuscript.

\vspace{0.5cm}
\noindent \textbf{Data Sharing:} The data is publicly available
through the original publications\cite{Bendall2011,Qiu2011} in the
following website: \url{http://reports.cytobank.org/1/v1}.

% For future work, So, we have N experiments: N+1 matrices 100k by 50: B
% (baseline), $E_1,...,E_N$. We want to use the baseline information to
% predict the funcional markers at different experiments (see below for
% more details) (also, surface markers do not change). That is, use B to
% predict $E_i$ with a neural network. Using the individual 1 as
% training, we can then only get baseline information for future
% individuals and predict their functional markers. This is great
% because this experiments are expensive, so our method would be widely
% used to save money.





%\input{references} %%originally referenc
\bibliography{neural-nets}
\bibliographystyle{nihunsrt} % Use the custom nihunsrt bibliography style included with the template

\end{document}
