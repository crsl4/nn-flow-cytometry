%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% track changes
\usepackage[normalem]{ulem}
\usepackage[dvipsnames]{xcolor}
\newcommand{\claudia}[2]{\sout{#1}\textcolor{magenta}{\textbf{#2}}}
\newcommand{\missing}[1]{\textcolor{red}{\textbf{#1}}}


\begin{document}

\title*{Prediction of functional markers of mass cytometry data via
  deep learning}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
%\thanks{Both authors contributed equally to this manuscript}
\author{Claudia Sol\'is-Lemus, Xin Ma, Maxwell Hostetter II, Suprateek
  Kundu, Peng Qiu, Daniel Pimentel-Alarc\'on}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Claudia Sol\'is-Lemus \at Emory University, Atlanta, GA, \email{csolisl@emory.edu}
\and Xin Ma \at Emory University, Atlanta, GA,
\email{xin.ma@emory.edu}
\and Maxwell Hostetter II \at Georgia State University, Atlanta, GA,
\email{mhostetter1@student.gsu.edu}
\and Suprateek Kundu \at Emory University, Atlanta, GA,
\email{suprateek.kundu@emory.edu}
\and Peng Qiu \at Georgia Institute of Technology, Atlanta, GA,
\email{peng.qiu@bme.gatech.edu}
\and Daniel Pimentel-Alarc\'on \at Georgia State University, Atlanta, GA,
\email{pimentel@gsu.edu}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\abstract{add here}

\section{Introduction}

% from Peng Qui PQ.docx: ---
Multiparametric single-cell analysis has advanced our understanding of
diverse biological and pathological processes, providing insights into
cellular differentiation, intracellular signaling cascades and
clinical immunophenotyping. Modern flow cytometers typically provide
simultaneous single-cell measurements of up to 12 fluorescent
parameters in routine cases, and analysis of up to 30 protein
parameters has been recently made commercially available. In addition,
a next-generation mass cytometry platform (CyTOF) has become
commercially available, which allows routine measurement of 50 or more
protein markers.

Despite the technological advances in acquiring an increasing number
of parameters per single cell, approaches for analyzing such complex
data lag behind. The existing approaches are often subjective and
labor-intensive. For example, the widely used gating approach
identifies cell types by user-defined sequences of nested 2-D
plots. There have been efforts to develop clustering algorithms (e.g.,
flowMeans\missing{(reference)}, flowSOM\missing{(reference)},
X-shift\missing{(reference)}) and dimension reduction algorithms
(e.g., SPADE\missing{(reference)}, tSNE\missing{(reference)},
Scaffold\missing{(reference)}). However, there is still huge space
for developing new methods to ask new questions in this field.
% ---

Recently, deep learning models are revolutionizing the fields of
precision medicine, data mining, astronomy, human-computer
interactions, among many others, by becoming a major discovery force
in science due to the unprecedented accuracy in prediction.  Moreover,
deep learning approaches have shown accurate performance on genomics
and biomedical
applications\cite{Ciresan2013,Denas2013,Fakoor2013,Leung2014,CruzRoa2014,Li2017,Mobadersany2018}.

Furthermore, CyTOF data is perfectly suited for deep learning methods.
On one side, identify markers define a cell type (e.g., B cell, T
cell, monocytes, MSC), and on the other side, expressions of
functional markers identify the cell's activity (e.g., quiescent,
secreting cytokines, proliferating, apoptosis). Since CyTOF technology
allows for the simulteanous measurement of a large number of protein
markers, most CyTOF studies measure both identity markers and
functional markers, providing data for supervised learning tools, like
neural networks.  In addition, each CyTOF run typically collects data
on $10^6$ cells, creating an ideal large dataset in which the number
of samples (cells) is orders of magnitued larger than the number of
variables (markers). Deep learning methods are particularly suited for
this type of big data.


Here, we explore neural network models to predict functional markers
(internal phosphoproteins) with identify markers (cell surface
proteins), and compare its performance in terms of accuracy and speed
to other standard statistical approaches like regression, and random
forests.  We show that neural networks improve prediction of
functional markers, making them a powerful alternative to the usual
regression techniques.



\section{Data}

\subsection{Pre-processing}

The dataset has been previously published in
\cite{Bendall2011,Qiu2011}. It contains measurements for 13 identity
(surface) markers, and 18 functional markers, which were performed on
5 healthy untreated donor bone marrow samples. The data roughly
contains 250,000 cells per sample (1,223,228 cells in
total). Thus, the data can be expressed in a $1223228 \times 31$
matrix.

The data was transformed with inverse hyperbolic sine function
($arcsinh$ with co-factor of 5), which is the standard transformation
for CyTOF data\missing{(reference)}.

\missing{(Peng: what was done to the data prior to sharing with us?)}

\subsection{Exploratory analysis}

We will compare the performance different methods (explained next
section) to predict the functional markers with the surface markers.
The data is highly complex and correlated, violating some of the
fundamental assumptions of standard statistical approaches (like
regression). For example, the data is highly skewed and the pattern
between response and predictor is not linear (see figure
\ref{exploratory}). 

\begin{figure}
\includegraphics[scale=0.3]{figures/surface-hist.pdf}
\includegraphics[scale=0.3]{figures/functional-hist.pdf}
\includegraphics[scale=0.3]{figures/scatterplot.pdf}
\caption{Exploratory plots of surface and functional markers. The
  histograms show a biased pattern, and the scatterplot shows non
  linearity, both violations of crucial assumptions in standard
  regression models.}
\label{exploratory}
\end{figure}


\section{Materials and Methods}

\subsection{Background on neural networks}

A neural network model is formed by several {\em neurons}. Each neuron
receives an input vector $x$, then weights its components according to
the neuron's weight vector $w$, adds a {\em bias} constant $b$, and
passes the result through a non-linear {\em activation function}
$\sigma$. This way, the output of a neuron is given by $\sigma(w^Tx +
b)$. There are several options for the activation function
$\sigma$. Common choices include the {\em sigmoid} function
$\sigma(z)=\frac{1}{1+e^-z}$ or the {\em rectified linear unit} (ReLU)
$\sigma(z)=\max(0,z)$. For the CyTOF data, we use the hyperbolic
tangent as activation function, as it showed better performance than
the sigmoid or ReLU functions (more details on the specific neural
network fit in subsection \ref{comparison}).

The final output of the network is given by $\hat{f}(x)$ with
parameters $\mathbf{W}_1,\cdots,\mathbf{W}_L$ for the weight matrices
and $b_1,\cdots,b_L$ for the bias vectors for each layer.

The estimation of the parameters is done through the following
optimization
\begin{equation}
\min_{\{W_l,b_l\}_{l=1}^L} \ \ \sum_{i=1}^n \| y_i-\hat{f}(x_i) \|^2.
\end{equation}

The most widely used technique to solve this optimization is through
stochastic gradient descent (SGD) and back-propagation, but we
discovered that Adam\cite{Kingma2014}, an algorithm for first-order
gradient-based optimization of stochastic objective functions, based
on adaptive estimates of lower-order moments had better performance
for our data (more details in subsection \ref{comparison}).


\subsection{Methods comparison}
\label{comparison}

We fit a neural network model to predict functional markers from
surface markers, and compare its performance to three classical
statistical methods: 1) linear regression (unpenalized and penalized),
2) decision trees, and 3) random trees. Due to computational time
constraints, we could not fit a support vector regression (SVR)
model. We compared the performance of the four approaches by computing
the mean square error (MSE) of the predicted responses.

To fit the models, we divided the data into training set, validation
set and testing set. The training set was used to fit each of the four
models. The validation set was used to determine the best setup
(tuning parameters) of each model in terms of MSE. Finally, the test
set was used to compare the MSE across methods.

The complete data consisted of 1,223,228 cells in 18 functional
markers (responses) and 15 surface markers (predictors), which we
divided as follows: 250,000 rows as training set, 250,000 rows as
validation set, and 223,228 rows as testing set.

We used two separate measure of performance: a vector MSE (equation
\ref{vectorMSEformula}) and individual MSE (equation
\ref{MSEformula}), one per predictor (so, 18 in total).

The vector MSE is defined as
\begin{equation}
MSE_{vec} = \frac{1}{n} \sum_{i=1}^n ||\hat{Y}_i - Y_i ||^2_2
\label{vectorMSEformula}
\end{equation}
where $\hat{Y}_i \in \mathbf{R}^{18}$ is the predicted vector of
responses for individual $i$, and $Y_i \in \mathbf{R}^{18}$ is the
observed vector of responses for individual $i$.

The individual MSE for predictor $k$ is defined  as
\begin{equation}
MSE_{(k)} = \frac{1}{n} \sum_{i=1}^n (\hat{Y}_{k,i} - Y_{k,i} )^2
\label{MSEformula}
\end{equation}
where $\hat{Y}_{k,i} \in \mathbf{R}$ is the $k^{th}$ predicted
response ($k=1,\cdots,18$) for individual $i$, and $Y_{k,i} \in
\mathbf{R}$ is the $k^{th}$ observed response for individual $i$.

% https://github.com/pluskid/Mocha.jl/blob/master/docs/user-guide/solver.rst
\vspace{0.25cm}
\noindent \textbf{Neural network model:} We tested different network
architectures, activation functions, regularization coefficients,
solver methods, momentum policies, and learning rates. The best
network has four layers (see figure \ref{nn}) with 90, 90, 45 and 45
nodes. The network uses hyperbolic tangent as activation function,
regularization coefficient of 0.0001, momentum policy at 0.8, learning
rates of 0.01, 0.0001, 0.75. We used Adam solver\cite{Kingma2014}, an
algorithm for first-order gradient-based optimization of stochastic
objective functions, based on adaptive estimates of lower-order
moments, instead of Stochastic Gradient Descent (SGD) as the former
showed increased accuracy. All networks were trained using the julia
package Mocha\missing{(reference)}.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/neuralNet.eps}
\caption{Neural network for predicting functional markers (18
  responses) from surface markers (15 predictors) with 4 hidden
  layers with 90, 90, 45 and 45 nodes each.}
\label{nn}
\end{figure}

\vspace{0.25cm}
\noindent \textbf{Linear regression (unpenalized/penalized):} We fit
standard linear regression, as well as the penalized version with
LASSO penalty under different penalization parameters. We used the
\textit{ScikitLearn}\missing{(reference)} julia wrapper, with default
settings. We noted that the penalized version performed worse than the
unpenalized version for all the predictors (regardless of penalty
parameter), so we only present results below for the unpenalized
linear regression model.

\vspace{0.25cm}
\noindent \textbf{Decision tree and random trees:} We fit one decision
regression tree with \textit{ScikitLearn} julia wrapper, with default
settings. We compared the performance of the ``mse'' criterion and the
Friedman's improvement score, deciding on the former (``mse'') which
is the default setting. We did not constraint the maximum depth of the
tree, and set as 2 the minimum number of samples required to split an
internal node. In addition, we did not constraint the maximum number
of features to consider when looking for the best split. Later, we fit
20 trees into a random forest method. We could explore more than 20
trees due to computational time constraints.



\section{Results}

Figure \ref{vectorMSE} (left) shows the vector MSE (equation
\ref{vectorMSEformula}) across all four different methods, being
decision tree the least accurate and neural network the most
accurate. Figure \ref{vectorMSE} (right) shows a comparison on
computation time (in seconds) among the four methods, being linear
regression the fastest and random forest the slowest. To sum up, the
neural network approach outperforms the other three methods in terms
of prediction accuracy, without sacrificing too much computational
speed.


\begin{figure}
\centering
\includegraphics[scale=0.35]{figures/vector-mse.pdf}
\includegraphics[scale=0.35]{figures/time.pdf}
\caption{Left: Vector MSE (equation \ref{vectorMSEformula}) for all
  four methods sorted from most accurate (neural network) to least
  accurate (decision tree). Right: Running time (in seconds) for the
  training and validation sets (sample 750,000 rows) for all four
  methods, sorted from fastest (linear regression) to slowest (random
  forest).}
\label{vectorMSE}
\end{figure}


Figure \ref{MSE} shows the individual MSE (equation \ref{MSEformula})
per predictor (18 predictors in x-axis) for each of the four
methods. Again, the prediction accuracy of neural network is better
than the other three methods for all the 18 predictors.

\begin{figure}
\centering
\includegraphics[scale=0.45]{figures/mse.pdf}
\caption{Individual MSE (equation \ref{MSEformula}) for each 18
  predictors. The neural network outperforms all other methods across
  all predictors.  Lines are drawn simply for visual effect.}
\label{MSE}
\end{figure}

Finally, we present a figure of the predicted response vs the observed
response, for all the four methods and 18 functional markers (figure
\ref{predObs}).

\begin{figure}
\centering
%\includegraphics[scale=0.4]{figures/neuralNet.eps}
\caption{Predicted vs observed responses across all functional markers
  and all four methods. The closer the slope to 1, the
  better.\missing{(missing this plot)}.}
\label{predObs}
\end{figure}



\section{Discussion}

In this work, we showed that a neural network model outperforms
standard statistical approaches like linear regression and random
forest in the prediction of functional markers from surface markers
for CyTOF data. Neural networks were also faster and more efficient
than random forests, which make them a more viable choice for big
datasets.

The improved prediction accuracy of neural networks can be explained
by their flexibility to account for non-linearity or skewness.  Unlike
regression models, neural networks do not have linearity or normality
assumptions, and they take advatange of the correlation structure
among responses by fitting a network for the whole response vector.

As mentioned already, CyTOF data is perfectly suited for deep learning
methods given the simulteanous measurement of a large number of
protein markers, including both both identity markers and functional
markers. Both measurements allow for the implementation of highly
accurate supervised methods, like neural networks. In addition, the
structure of CyTOF data is ideal for deep learning: number of samples
orders of magnited greater than the number of variables.

For future work, we can include an extended version of the
dataset\cite{Bendall2011,Qiu2011} that includes 24 healthy sample of
bone marrow treated by 24 different drugs. In this setting, we are
interested in predicting the functional markers under different drug
scenarios, using information at baseline (no treatment) and surface
markers at different treatment levels.
Furthermore, based on the trained deep learning model, we are
interested in the question of whether we can identify cell clusters,
and whether these cell clusters agree with well-accepted cell types in
literature.
Finally, if we focus on cells belonging to the same known cell type, and
examine the distribution of functional markers and the correlation
with the subtle variations of the identity markers among cells of this
type, we can explore whether there is evidence that the specific cell
type could be further divided into subtypes.



% For future work, So, we have N experiments: N+1 matrices 100k by 50: B
% (baseline), $E_1,...,E_N$. We want to use the baseline information to
% predict the funcional markers at different experiments (see below for
% more details) (also, surface markers do not change). That is, use B to
% predict $E_i$ with a neural network. Using the individual 1 as
% training, we can then only get baseline information for future
% individuals and predict their functional markers. This is great
% because this experiments are expensive, so our method would be widely
% used to save money.





%\input{references} %%originally referenc
\bibliography{neural-nets}
\bibliographystyle{nihunsrt} % Use the custom nihunsrt bibliography style included with the template

\end{document}
